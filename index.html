<!DOCTYPE html>  
<html>  

<head>  
  <meta charset="utf-8">  
  <meta name="google-site-verification" content="lTNLmXZvHm_RXq9IklCb7g-0DA_3JIIpvcV9Um-vpRQ" />
  <!-- Replace the content tag with appropriate information -->  
  <meta name="description" content="RedCode official webpage">  
  <meta property="og:title" content="RedCode" />  
  <meta property="og:description" content="RedCode official webpage" />  

  <title>RedCode: Multi-Dimensional Safety Benchmark for Code Agents</title>  
  <link rel="icon" type="image/x-icon" href="assets/RedCode-logo-1024.ico">  
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">  
  <link href="https://fonts.googleapis.com/css2?family=Nanum+Gothic+Coding:wght@400;700&display=swap" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css2?family=Ubuntu+Mono&display=swap" rel="stylesheet">  
  <link href="https://fonts.googleapis.com/css2?family=Chivo+Mono:ital,wght@0,100..900;1,100..900&display=swap" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">  
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">  
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">  
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">  
  <link rel="stylesheet" href="static/css/index.css">  
  <link rel="stylesheet" href="static/css/navbar.css">  
  <link rel="stylesheet" href="static/css/redcode-font.css">  
  <link rel="stylesheet" href="static/css/leaderboard.css">  

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>  
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>  
  <script defer src="static/js/fontawesome.all.min.js"></script>  
  <script src="static/js/bulma-carousel.min.js"></script>  
  <script src="static/js/bulma-slider.min.js"></script>  
  <script src="static/js/index.js"></script>  

</head>  

<body>  

  <nav class="navbar"> 
    <div class="navbar-left">
        <a href="#home">
            <img src="assets/RedCode-logo-512.png" alt="RedCode Logo">
            <span class="name-bold">RedCode</span>
        </a>
    </div>

    <div class="navbar-right" id="navbar-right">
        <a href="#home">Home</a>
        <a href="#abstract">Abstract</a>
        <a href="#benchmark">Overview</a>
        <a href="#leaderboard">Leaderboard</a>
        <a href="#results">Results Demo</a>
        <a href="#BibTeX">Cite us</a>
    </div>
    
    <div class="menu-icon">
        <i class="fa fa-bars"></i>
    </div>
  </nav>


  <section class="hero" id="home">  
    <div class="hero-body">  
      <div class="container is-max-desktop">  
        <div class="columns is-centered">  
          <div class="column has-text-centered">  
            <h1 class="title is-1 publication-title"><img src="assets/RedCode-logo-512.png" style="vertical-align: middle" width="50px"> <span class="name-bold">RedCode</span>: Multi-Dimensional Safety Benchmark for Code Agents</h1>  
            <div class="is-size-5 publication-authors">  
              <!-- Paper authors -->  
              <span class="author-block">  
                <a href="https://github.com/1mocat" target="_blank">Chengquan Guo</a><sup>1*</sup>,</span>  
              <span class="author-block">  
                <a href="https://antiquality.github.io" target="_blank">Xun Liu</a><sup>2*</sup>,</span>  
              <span class="author-block">  
                <a href="https://alphapav.github.io" target="_blank">Chulin Xie</a><sup>3*</sup>,</span>  
              <br>  
              <span class="author-block">  
                <a href="https://www.andyzhou.ai" target="_blank">Andy Zhou</a><sup>3,4</sup>,</span>  
              <span class="author-block">  
                <a href="https://www.yi-zeng.com" target="_blank">Yi Zeng</a><sup>5</sup>,</span>  
              <span class="author-block">  
                <a href="https://zinanlin.me" target="_blank">Zinan Lin</a><sup>6</sup>,</span>  
              <span class="author-block">  
                <a href="https://dawnsong.io" target="_blank">Dawn Song</a><sup>7</sup>,</span>  
              <span class="author-block">  
                <a href="https://aisecure.github.io" target="_blank">Bo Li</a><sup>3,8</sup>  
              </span>  
            </div>  

            <div class="is-size-5 publication-authors">  
              <span class="author-block">  
                <small>  
                  <sup>1</sup>Zhejiang University  
                  <sup>2</sup>University of Chinese Academy of Sciences  
                  <sup>3</sup>UIUC  
                  <br>  
                  <sup>4</sup>Lapis Labs  
                  <sup>5</sup>Virginia Tech  
                  <sup>6</sup>Microsoft Corporation  
                  <sup>7</sup>UC Berkeley  
                  <sup>8</sup>University of Chicago  
                </small>  
              </span>  
              <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>  
            </div>  

            <div class="column has-text-centered">  
              <div class="publication-links">  
                <!-- Arxiv PDF link -->  
                <span class="link-block">  
                  <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank" class="external-link button is-normal is-rounded is-dark">  
                    <span class="icon">  
                      <i class="fas fa-file-pdf"></i>  
                    </span>  
                    <span>Paper</span>  
                  </a>  
                </span>  

                <!-- ArXiv abstract Link -->  
                <span class="link-block">  
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank" class="external-link button is-normal is-rounded is-dark">  
                    <span class="icon">  
                      <i class="ai ai-arxiv"></i>  
                    </span>  
                    <span>arXiv</span>  
                  </a>  
                </span>  

                <!-- Github link -->  
                <span class="link-block">  
                  <a href="https://github.com/RedCode-2024/RedCode" target="_blank" class="external-link button is-normal is-rounded is-dark">  
                    <span class="icon">  
                      <i class="fab fa-github"></i>  
                    </span>  
                    <span>Code</span>  
                  </a>  
                </span>  

                <!-- Dataset link -->  
                <span class="link-block">  
                  <a href="https://huggingface.co/datasets/AI-Secure/RedCode" target="_blank" class="external-link button is-normal is-rounded is-dark">  
                    <span class="icon">
                        <img src="assets/huggingface-logo.svg" alt="Hugging Face Logo" width="250" height="250">  
                    </span>
                    <span>Dataset</span>  
                  </a>  
                </span>  
                
              </div>  
            </div>  
          </div>  
        </div>  
      </div>  
    </div>  
  </section>  

  <!-- Teaser video-->  
  <section class="hero teaser">  
    <div class="container is-max-desktop">  
      <div class="hero-body">  
        <h2 class="subtitle has-text-centered">  
          Our experiment reveals that
          <br>  
          <b>OpenCodeInterpreter is safer than ReAct and CodeAct agents.</b>  
          <br>  
          <img src="assets/fig-1(1x1)-rectangle.png" alt="Fig-1:scatter of REJ-ASR." style="width: 80%; height: auto;"/>  
          <br>
        </h2>  
        <div class="columns is-centered has-text-centered">  
          <p style="color: gray;">
            Safety evaluation of 13 code agents on <span class="name-context">RedCode-Exec</span>, where the color of the datapoint
            denotes agent type and the label "A(B)" denotes the results of this agent with base LLM A under risky test cases in
            language B, and * denotes fine-tuned LLMs for agent. The more upper left the datapoint is located, the safer the agent.
            In general, OpenCodeInterpreter series (the yellow ones) are the safest agents, ReAct series (the red ones) are in the
            middle, and CodeAct series (the blue ones) are the most unsafe.
            Different base LLMs significantly affect the agent's safety.
          </p>
        </div>
      </div>  
    </div>  
  </section>  
  <!-- End teaser video -->  

  <!-- Paper abstract -->  
  <section class="section hero is-light" id="abstract">  
    <div class="container is-max-desktop">  
      <div class="columns is-centered has-text-centered">  
        <div class="column is-four-fifths">  
          <h2 class="title is-3">Abstract</h2>  
          <div class="content has-text-justified">  
            <p>  
              <p>
                <b>TL;DR: Our high-quality large-scale dataset offers holistic evaluation on Code Agents, in diverse languages and formats.</b>
              </p>
              With the rapidly increasing capabilities and adoption of code agents for AI-assisted coding and software development, safety and security concerns - such as generating or executing malicious code - have become significant barriers to the real-world deployment of these agents. To provide comprehensive and practical evaluations on the safety of code agents, we propose <span class="name-context">RedCode</span>,  
              an evaluation platform with benchmarks grounded in four key principles - real interaction with systems, holistic evaluation of unsafe code generation and execution, diverse input formats, and high-quality safety scenarios and tests.  
              <span class="name-context">RedCode</span> consists of two parts to evaluate agents' safety in unsafe code execution and generation:  
              <b>  
              (1) <span class="name-context">RedCode-Exec</span> provides challenging code prompts in Python as inputs, aiming to evaluate code agents' ability to recognize and handle unsafe code. We then map the Python code to other programming languages (e.g., Bash) and natural text summaries or descriptions for evaluation, leading to a total of over 4,000 testing instances.  
              </b>  
              We provide 25 types of critical vulnerabilities spanning various domains, such as websites, file systems, and operating systems.  
              We provide a Docker sandbox environment to evaluate the execution capabilities of code agents and design corresponding evaluation metrics to assess their execution results.  
              <b>  
              (2) <span class="name-context">RedCode-Gen</span> provides 160 prompts with function signatures as input to assess whether code agents will follow instructions to generate harmful code or software.  
              </b>  
              Our empirical findings, derived from evaluating three agents based on various LLMs, provide insights into code agents' vulnerabilities. For instance,  evaluations on <span class="name-context">RedCode-Exec</span> show that agents are more likely to reject executing unsafe operations on operating system. Unsafe operations described in natural text lead to a lower rejection rate than those in code format.  
              Additionally,  evaluations on <span class="name-context">RedCode-Gen</span> reveal that  
              more capable base models and agents with stronger overall coding abilities, such as GPT-4, tend to produce more sophisticated and effective harmful software.  
              Our findings highlight the need for stringent safety evaluations for diverse code agents.  
            </p>  
          </div>  
        </div>  
      </div>  
    </div>  
  </section>  
  <!-- End paper abstract -->  

  <!-- Benchmark overview -->  
  <section class="section hero is-small" id="benchmark">  
    <div class="container is-max-desktop">  
      <div class="columns is-centered has-text-centered">  
        <div class="column is-four-fifths">  
          <h2 class="title is-3">Benchmark Overview</h2>  
          <div class="content has-text-justified">  
            <p>
              <p>
                <span class="name-context">RedCode</span> consists of two parts to evaluate agents' safety in unsafe code execution and
                generation: <span class="name-context">RedCode-Exec</span> and <span class="name-context">RedCode-Gen</span>.
              </p>
              <img src="assets/dataset-pipeline-new-5.jpg" alt="Scatter of REJ-ASR." style="margin-bottom: 10px;"/>
              <br>
              <div class="columns is-centered has-text-centered">
                <p style="color: gray;">
                  Overview of <span class="name-context">RedCode</span> benchmark.
                  The <span class="name-context">RedCode-Exec</span> component is designed to assess risks associated with code
                  execution, while <span class="name-context">RedCode-Gen</span> component evaluates risks in software generation.
                </p>
              </div>
              <p>
                For <span class="name-context">RedCode-Exec</span>, we begin by collecting the risky scenarios from the existing benchmarks and CWE. Guided by these risky
                scenarios and risky descriptions, we construct initial seed test cases (python code snippets) by manually designing
                the code or gathering from online sources. Next, we employ a human-in-the-loop procedure to expand and refine our dataset.
                <br>
                <br>
                For <span class="name-context">RedCode-Gen</span>, we provide eight risky categories based on well-studied malware families for risky software generation.
                The prompts are generated with human-in-the-loop AI-assisted generation. We then give the prompts to the agent to
                generate code in our docker environment. We evaluate the quality of the generated code via both LLM-as-a-judge and
                VirusTotal API.
              </p>
              <img src="assets/25-category.jpg" alt="Taxonomy of RedCode-Exec."/>
              <br>
              <div class="columns is-centered has-text-centered">
                <p style="color: gray;">
                  Taxonomy of 25 risky scenarios spanning 8 domains in <span class="name-context">RedCode-Exec</span>, ranging from real system, network operations to program logic and so on.
                </p>
              </div>
            </p>
          </div>  
        </div>  
      </div>  
    </div>  
  </section>  
  <!-- End Benchmark overview -->  

  <!-- Leaderboard -->  
  <section class="section hero is-light" id="leaderboard">  
    <div class="container is-max-desktop">  
      <div class="columns is-centered has-text-centered">  
        <div class="column is-four-fifths">  
          <h2 class="title is-3">Leaderboard</h2>  
          <div class="content has-text-justified">  
            <p>  
              The results of OCI, ReAct and CodeAct on <span class="name-context">RedCode-Exec</span> and <span class="name-context">RedCode-Gen</span> are shown in the following tables.
            </p>  

            <table id="leaderboard-exec">
                <thead>
                    <tr>
                        <th>Rank</th>
                        <th>Model</th>
                        <th>Agent</th>
                        <th>Task</th>

                        <th id="sort-rejection-exec" class="highlight tooltip">Rejection(%) ↑
                          <span class="sort-indicator" id="rejection-indicator-exec">▼</span>
                          <span class="tooltiptext">Sort</span>
                        </th>
                        <th id="sort-asr-exec" class="highlight tooltip">ASR(%) ↓
                          <span class="sort-indicator" id="asr-indicator-exec"></span>
                          <span class="tooltiptext">Sort</span>
                        </th>
                    </tr>
                </thead>
                <tbody>
                    <!-- Rows will be inserted here by JavaScript -->
                </tbody>
            </table>

          </div>  
        </div>  
      </div>  
    </div>  
  </section>  

  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script src="static/js/leaderboard.js"></script>
  <!-- End Leaderboard -->  

  <!-- Results Demo -->  
  <section class="section hero is-small" id="results">  
    <div class="container is-max-desktop">  
      <div class="columns is-centered has-text-centered">  
        <div class="column is-four-fifths">  
          <h2 class="title is-3">Results Demo</h2>  
          <div class="content has-text-justified">  
            <p>  
              <p>
                The heatmap and radar figures show the performance of the three agents on <span class="name-context">RedCode-Exec</span> benchmark.
              </p>
              <img src="assets/radar-high-level.jpg" alt="Radar figure of RedCode-Exec."/>
              <div class="columns is-centered has-text-centered">
                <p style="color: gray;">
                  Reject and attack success rates of 8 representative code agents on <span class="name-context">RedCode-Exec</span> Python test cases
                  in 8 domains. The overall attack success rate is high, indicating existing code agents are vulnerable. Agents
                  achieve a higher rejection rate on risky cases in operation and file systems than those in other domains.
                </p>
              </div>

              <img src="assets/heatmap.jpg" alt="Heatmap of RedCode-Exec."/>
              <div class="columns is-centered has-text-centered">
                <p style="color: gray;">
                  Rejection and attack success rates for <span class="name-context">RedCode-Exec</span> across 11 <span style="color: red;"> Note: Now we have 13. Update needed.</span>   different agents. OCI/CA/RA denotes OpenCodeInterpreter/CodeAct/ReAct agents, and  * denotes fine-tuned LLMs for corresponding agents.
                </p>
              </div>
            </p>  
          </div>  
        </div>  
      </div>  
    </div>  
  </section>  
  <!-- Results Demo -->  


  <!--BibTex citation -->  
  <section class="section is-light" id="BibTeX">  
    <div class="container is-max-desktop content">  
      <h2 class="title">BibTeX</h2>  
      <pre><code class="bibtex-code">BibTex Code Here</code></pre>  
    </div>  
  </section>  
  <!--End BibTex citation -->  

  <footer class="footer">  
    <div class="container">  
      <div class="columns is-centered">  
        <div class="column is-8">  
          <div class="content">  

            <p>
              Acknowledgement: This webpage is inspired by other excellent work, such as <a
              href="https://sorry-bench.github.io/index.html" target="_blank">SORRY-Bench</a>.
              <br>
              <br>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io"
                target="_blank">Nerfies</a> project page.
              You are free to borrow the of this website, we just ask that you link back to this page in the footer.
              <br>
              <br>
              This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"
                target="_blank">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>

          </div>  
        </div>  
      </div>  
    </div>  
  </footer>  

  <!-- Statcounter tracking code -->  

  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->  

  <!-- End of Statcounter Code -->  

  <script>  
    function toggleNavbar() {  
      var navbarRight = document.getElementById("navbar-right");  
      navbarRight.classList.toggle("active");  
    }  
  </script>

</body>  

</html>