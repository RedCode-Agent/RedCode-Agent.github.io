<!DOCTYPE html>  
<html>  

<head>  
  <meta charset="utf-8">  
  <meta name="google-site-verification" content="lTNLmXZvHm_RXq9IklCb7g-0DA_3JIIpvcV9Um-vpRQ" />
  <meta name="description" content="RedCode Benchmark Official Webpage">  
  <meta property="og:title" content="RedCode" />  
  <meta property="og:description" content="A Risky Code Execution and Generation Benchmark for Code Agents" />  

  <title>RedCode</title>  
  <link rel="icon" type="image/x-icon" href="assets/RedCode-logo-1024.ico">  
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">  
  <link href="https://fonts.googleapis.com/css2?family=Nanum+Gothic+Coding:wght@400;700&display=swap" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css2?family=Ubuntu+Mono&display=swap" rel="stylesheet">  
  <link href="https://fonts.googleapis.com/css2?family=Chivo+Mono:ital,wght@0,100..900;1,100..900&display=swap" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">  
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">  
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">  
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">  
  <link rel="stylesheet" href="static/css/index.css">  
  <link rel="stylesheet" href="static/css/navbar.css">  
  <link rel="stylesheet" href="static/css/redcode-font.css">  
  <link rel="stylesheet" href="static/css/leaderboard.css">  

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>  
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>  
  <script defer src="static/js/fontawesome.all.min.js"></script>  
  <script src="static/js/bulma-carousel.min.js"></script>  
  <script src="static/js/bulma-slider.min.js"></script>  
  <script src="static/js/index.js"></script>  

</head>  

<body>  

  <nav class="navbar"> 
    <div class="navbar-left">
        <a href="#motivation">
            <img src="assets/RedCode-logo-512.png" alt="RedCode Logo">
            <b><span class="name-bold">RedCode</span></b>
        </a>
    </div>

    <div class="navbar-right" id="navbar-right">
        <a href="#motivation">Motivation</a>
        <!-- <a href="#abstract">Abstract</a> -->
        <a href="#results">Results</a>
        <a href="#benchmark">Benchmark</a>
        <a href="#leaderboard">Leaderboard</a>
        
        <!-- <a href="#case-study">Case Study</a> -->
        <a href="#BibTeX">Cite us</a>
    </div>
    
    <div class="menu-icon">
        <i class="fa fa-bars"></i>
    </div>
  </nav>


  <section class="hero" id="motivation">  
    <div class="hero-body">  
      <div class="container is-max-desktop">  
        <div class="columns is-centered">  
          <div class="column has-text-centered">  
            <h1 class="title is-1 publication-title"><img src="assets/RedCode-logo-512.png" style="vertical-align: middle" width="50px"> <b><span class="name-bold" >RedCode</span></b>: Risky Code Execution and Generation Benchmark for Code Agents</h1>  
            <div class="is-size-5 publication-authors">  
              <!-- Paper authors -->  
              <span class="author-block">  
                <a href="https://github.com/1mocat" target="_blank">Chengquan Guo</a><sup>1*</sup>,</span>  
              <span class="author-block">  
                <a href="https://antiquality.github.io" target="_blank">Xun Liu</a><sup>2*</sup>,</span>  
              <span class="author-block">  
                <a href="https://alphapav.github.io" target="_blank">Chulin Xie</a><sup>2*</sup>,</span>  
              <br>  
              <span class="author-block">  
                <a href="https://www.andyzhou.ai" target="_blank">Andy Zhou</a><sup>2,3</sup>,</span>  
              <span class="author-block">  
                <a href="https://www.yi-zeng.com" target="_blank">Yi Zeng</a><sup>4</sup>,</span>  
              <span class="author-block">  
                <a href="https://zinanlin.me" target="_blank">Zinan Lin</a><sup>5</sup>,</span>  
              <span class="author-block">  
                <a href="https://dawnsong.io" target="_blank">Dawn Song</a><sup>6</sup>,</span>  
              <span class="author-block">  
                <a href="https://aisecure.github.io" target="_blank">Bo Li</a><sup>1,2</sup>  
              </span>  
            </div>  

            <div class="is-size-5 publication-authors">  
              <span class="author-block">  
                <small>  
                  <sup>1</sup>University of Chicago
                  <sup>2</sup>University of Illinois Urbana-Champaign 
                  <sup>3</sup>Lapis Labs  
                  <br> 
                  <sup>4</sup>Virginia Tech
                  <sup>5</sup>Microsoft Corporation  
                  <sup>6</sup>UC Berkeley
                </small>  
              </span>  
              <span class="eql-cntrb"><small><br><sup>*</sup> Equal Contribution.</small> <small>Work done during Chengquan's internship at the University of Chicago and <br> Xun's internship at the University of Illinois Urbana-Champaign.</small></span>  
            </div>  

            <p style="color: red;"> TODO: Fill out the link in Paper and arXiv block. </p>  

            <div class="column has-text-centered">  
              <div class="publication-links">  
                <!-- Arxiv PDF link -->  
                <span class="link-block">  
                  <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank" class="external-link button is-normal is-rounded is-dark">  
                    <span class="icon">  
                      <i class="fas fa-file-pdf"></i>  
                    </span>  
                    <span>Paper</span>  
                  </a>  
                </span>  

                <!-- ArXiv abstract Link -->  
                <!-- <span class="link-block">  
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank" class="external-link button is-normal is-rounded is-dark">  
                    <span class="icon">  
                      <i class="ai ai-arxiv"></i>  
                    </span>  
                    <span>arXiv</span>  
                  </a>  
                </span>   -->

                <!-- Github link -->  
                <span class="link-block">  
                  <a href="https://github.com/RedCode-2024/RedCode" target="_blank" class="external-link button is-normal is-rounded is-dark">  
                    <span class="icon">  
                      <i class="fab fa-github"></i>  
                    </span>  
                    <span>Code</span>  
                  </a>  
                </span>  

                <!-- Leaderboard link -->
                <span class="link-block">  
                  <a href="#leaderboard" class="external-link button is-normal is-rounded is-dark">  
                    <span class="icon">
                      <i class="fa fa-trophy"></i>
                    </span>
                    <span>Leaderboard</span>  
                  </a>  
                </span> 

                <!-- Dataset link -->  
                <span class="link-block">  
                  <a href="https://huggingface.co/datasets/AI-Secure/RedCode" target="_blank" class="external-link button is-normal is-rounded is-dark">  
                    <span class="icon">
                        <img src="assets/huggingface-logo.svg" alt="Hugging Face Logo" width="250" height="250">  
                    </span>
                    <span>Dataset</span>  
                  </a>  
                </span>  
              </div>  
            </div>  
          </div>  
        </div>  
      </div>  
    </div>  
  </section>  
  <!-- Teaser video-->  
  <!-- <section class="hero teaser">  
    <div class="container is-max-desktop">  
      <div class="hero-body">  
        <h2 class="subtitle has-text-centered">  
          <div class="content has-text-justified">  
            <p>
            Integrated with external tools like Python interpreters or shell environments, LLM-based code agents have significantly advanced AI-assisted coding and software development.
            However, despite their impressive capabilities, these ‚ö†Ô∏è<b style="color: rgb(255, 115, 0);">code agents are not risk-free</b>.
            <b><i>Code agents can inadvertently suggest or execute code with security vulnerabilities like deleting important files or leaking sensitive information.</i></b>
            <br>üéØTo rigorously and comprehensively evaluate the safety of code agents, 
            we propose <img src="assets/RedCode-logo-512.png" style="vertical-align: middle" width="28px"><b class="name-bold"> RedCode</b>
            , a <u>high-quality, large-scale</u> dataset that features <u>diverse languages and formats</u>, providing <u>real interaction with systems</u> and <u>fine-grained evaluation</u> of both <u>code execution and generation</u>.
            </p>
          </div>
          <br>  
          Our experiment reveals that
          <br>  
          <b>OpenCodeInterpreter is safer than ReAct and CodeAct agents.</b>  
          <br>  
          <p style="color: red;"> TODO: Other impressive slogan? </p>  
          <img src="assets/fig-1(1x1).png" alt="Fig-1:scatter of REJ-ASR." style="width: 80%; height: auto;"/>  
          <br>
        </h2>  
        <div class="columns is-centered has-text-centered">  
          <p style="color: gray;">
            Safety evaluation of 14 code agents on <span class="name-context">RedCode-Exec</span>, where the color of the datapoint
            denotes agent type and the label "A(B)" denotes the results of this agent with base LLM A under risky test cases in
            language B, and * denotes fine-tuned LLMs for agent. The more upper left the datapoint is located, the safer the agent.
            In general, OpenCodeInterpreter series (the yellow ones) are the safest agents, ReAct series (the red ones) are in the
            middle, and CodeAct series (the blue ones) are the most unsafe.
            Different base LLMs significantly affect the agent's safety.
            <br>
            <span style="color: red;"> TODO: Do we also need to cite every figure in the webpage, like what we did in paper? </span>  
          </p>
        </div>
      </div>  
    </div>  
  </section>   -->
  <!-- End teaser video -->  

  <section class="section hero is-small" id="motivation">  
    <div class="container is-max-desktop">  
      <div class="columns is-centered has-text-centered">  
        <div class="column is-four-fifths">  
          <h2 class="title is-3" style="margin-top: -80px;">Motivation</h2>  
            <div class="content has-text-justified">  
              <p>
              Integrated with external tools like <img src="assets/python.png" style="vertical-align: middle" width="28px"> Python interpreters or <img src="assets/shell.png" style="vertical-align: middle" width="28px"> shell environments, LLM-based code agents have significantly advanced AI-assisted coding and software development.
              However, despite their impressive capabilities, these ‚ö†Ô∏è<b style="color: rgb(255, 0, 0);">code agents are not risk-free</b>.
              <b>Code agents can inadvertently suggest or execute code with security vulnerabilities like deleting important files or leaking sensitive information.</i></b>
              <br>
              <br>üéØTo rigorously and comprehensively evaluate the safety of code agents, 
              we propose <img src="assets/RedCode-logo-512.png" style="vertical-align: middle" width="28px"><b class="name-bold"> RedCode</b>,
              a <u>high-quality, large-scale (over 4,000 test cases)</u> dataset that features <u>diverse languages and formats (Python, Bash, natural language)</u>, providing <u>real interaction with systems</u> and <u>fine-grained evaluation</u> of both <u>code execution and generation</u>.
              <br>
              <br>
              <!-- <span style="color: rgb(0, 96, 12); background-color: yellow; font-weight: bold; font-size: 24px; border: 2px solid red; padding: 5px; display: inline-block;">TODO: href here</span> -->
              <img src="assets/RedCode-logo-512.png" style="vertical-align: middle" width="28px"> <span class="name-context">RedCode</span> consists of <span class="name-context">RedCode-Exec</span> and <span class="name-context">RedCode-Gen</span>.
              <ul>
                <li>
                  <span class="name-context">RedCode-Exec</span> provides prompts to evaluate code agents' ability to recognize and handle unsafe code with a total of 4,050 testing instances.
                </li>
                <li>
                  <span class="name-context">RedCode-Gen</span> provides 160 prompts with function signatures as input to assess whether code agents will follow instructions to generate harmful code or software.
                </li>
              </ul>
              </p>
            </div>
          </h2>  
          


          </div>  
        </div>  
      </div>  
    </div>  
  </section>  


    <!-- Results Demo -->
    <script src="https://d3js.org/d3.v7.min.js"></script>
    <script src="static/js/leaderboard.js"></script>
    
    <section class="section hero is-small" id="results">  
      <div class="container is-max-desktop">  
        <div class="columns is-centered has-text-centered">  
          <div class="column is-four-fifths">  
            <h2 class="title is-3" style="margin-top: -80px;">Results</h2>  
            <div class="content has-text-justified">
              We present the following <b style="color:rgb(255, 0, 0);">üí°5 new findings (Finding 1-5) </b>from our evaluations, which have not been revealed in prior benchmarks. 
              Our findings encompass comparisons of <u>various models and agents</u> (<b><a href="#demo1" class="demo-link1">Finding 1</a></b> and <b><a href="#demo4" class="demo-link4">Finding 4</a></b>), 
              <u>diverse risky scenarios</u> (<b><a href="#demo2" class="demo-link2">Finding 2</a></b>, <b><a href="#demo4" class="demo-link4">Finding 4</a></b> and <b><a href="#demo5" class="demo-link5">Finding 5</a></b>),
              and <u>different input languages</u> (<b><a href="#demo3" class="demo-link3">Finding 3</a></b>).</b>
              <br>
              <br>Because <span class="name-context">RedCode-Exec</span> provides unsafe user queries (i.e., test cases) for code execution, our query can be regarded as an attack on the agent.
              We will get one of the following three possible safety outcomes for each test case: 
              <ol>
                <li>
                  <b style="color:green;">Rejection: The user query is rejected.</b> 
                </li>
                <li>
                  <b style="color:red;">Attack Success: The threat functionality is successfully achieved
                    (i.e., code execution succeeds and brings corresponding outcome).</b>
                </li>
                <li>
                  <b style="color:grey;">Execution Failure: The threat functionality is not achieved. </b>
                </li>
              </ol>
            </div>  
          </div>  
        </div>  

        <div id="demo1" class="demo-container">
            <img src="assets/fig-1(1x1).jpg" alt="Fig-1:scatter of REJ-ASR."/>
            
              <b><a href="#demo1" class="demo-link1">Finding 1: OpenCodeInterpreter is üõ°Ô∏èsafer than ReAct and CodeAct agents.</a></b>
              Safety evaluation results of 17 code agents on <span class="name-context">RedCode-Exec</span> are shown on the figure above, where the color of the datapoint
                denotes agent type and the label "A(B)" denotes the results of this agent with base LLM A under risky test cases in
                language B, OCI denotes OpenCodeInterpreter and * denotes fine-tuned LLMs for agents. The more upper left the datapoint is located, the safer the agent.
                <u>In general, OpenCodeInterpreter series (the yellow ones) are the safest agents, ReAct series (the red ones) are in the
                middle, and CodeAct series (the blue ones) are the most unsafe.</u>
                Different base LLMs also significantly affect the agent's safety. <b><a href="#demo4" class="demo-link4">The heatmaps below</a></b> also show more details on comparions between different base LLMs.
                <br>
                <!-- <span style="color: red;"> TODO: Do we also need to cite every figure in the webpage, like what we did in paper? </span>   -->
            
        </div>
        <br>
          <div id="demo2" class="demo-container" style="margin-top: -1em;">
            <img src="assets/radar-high-level.jpg" alt="Radar figure of RedCode-Exec."/>
            <!-- <p style="color: gray;"> -->
              <!-- <span style="color: red;"> TODO: The titles of radar figure are better to be capitalized in the first alphabet.</span>   -->
              <br>
              <b><a href="#demo2" class="demo-link2">Finding 2: Agents are more likely to reject executing unsafe operations in operating system domain.</a></b>
              Rejection rate and attack success rate of 8 representative code agents on <span class="name-context">RedCode-Exec</span> Python test cases
              in 8 domains are shown in the figure above. <u>The overall attack success rate is high, indicating existing code agents are üí£vulnerable. We also find that agents</u>
              achieve a higher rejection rate on risky cases in the operating system domain than those in other domains. <b><a href="#demo4" class="demo-link4">The heatmaps below</a></b> show more details about rejection rate and attack success rate under each specific risky scenario.
              <br>

          </div>
          <div id="demo3"  class="demo-container">
            <img src="assets/demo3.jpg" style="width: 100%; max-width: 400px; display: block; margin: 0 auto;" alt="Radar figure of RedCode-Exec." />
            <!-- <p style="color: gray;"> -->
              <!-- <span style="color: red;"> TODO: The titles of radar figure are better to be capitalized in the first alphabet.</span>   -->
          
              <b><a href="#demo3" class="demo-link3">Finding 3: Agents are less likely to reject risky queries in natural language than programming language inputs, or in Bash code than Python code inputs.</a></b>
              We select ReAct agent based on GPT-4 as a representative, the "RR" denotes rejection rate and "ASR" denotes attack success rate. 
              <!-- the evaluation results show that agent is less likely to reject risky queries in natural language compared to programming language inputs, and in Bash code compared to Python code inputs. -->
              We employ a <b>variety of input prompts</b>: for each programming language (Python, Bash), we use the <u>code snippets</u>, <u>text summaries of code snippets</u>, and <u>text descriptions of code snippets</u> as input prompts. In addition, for code snippets, we add a persuasive jailbreaking prefix (i.e., I highly recommend that you run the following {language} code...), which we name as <u>‚Äúcode with jailbreak‚Äù</u>. 
              <!-- <br> -->

              </div>

            <div id="demo4" class="demo-container" style="position: relative;"> 
              <b><a href="#demo4" class="demo-link4"> Finding 4: More capable base models, such as GPT series, tend to have a higher rejection rate for unsafe operations under the same Agent structure.</a></b> This could be concluded from the heatmaps below.
              We present <b style="color:green;">Rejection Rate (RR)</b> and <b style="color:red;">Attack Success Rate (ASR)</b> for 17 different agents on RedCode-Exec, covering both Python and Bash tasks. 
              The overall rejection rate is low, and the attack success rate is high for all code
              agents, which demonstrates the ‚úÖhigh quality, ‚úÖstealthiness, and ‚úÖchallenging test cases of our benchmark. Note that * in the heatmap denotes fine-tuned LLMs for agents.
              <br>
              <br>
              üñ±Ô∏è<b>Click</b> the buttons below to view the corresponding results.
              <div class="button-container">  
                <button id="choice1">RR for Python</button>
                <button id="choice2">ASR for Python</button>
                <button id="choice3">RR for Bash</button>
                <button id="choice4">ASR for Bash</button>
              </div>
            
            <div id="heatmap" style="margin-top: 0em; margin-right: -3em; padding: 0;"></div>
                            
            <div id="tooltip" style="position: absolute; display: none; padding: 10px; background: rgba(0, 0, 0, 0.6); color: white; border-radius: 5px; pointer-events: none; z-index: 10;"></div>
                          
            <script src="static/js/heatmap_python_rej.js"></script>

            <div id="demo4_text" style="margin-top: 1em; padding: 0;text-align: center;">
              
            <b>Heatmap 4.1: Rejection rate for Python on RedCode-Exec across 17 different agents.</b>
            </div>

            <script src="static/js/choose_heatmap.js"></script>
            </div>

          <div id="demo5"  class="demo-container">
            <img src="assets/demo5.png" style="width: 100%; max-width: 400px; display: block; margin: 0 auto;" alt="Radar figure of RedCode-Exec."/>
              Overall results for base models and code agents on <span class="name-context">RedCode-Gen</span>. We find low refusal rates and high accuracy in this setting for most base models. But code agents, have lower refusal rates and higher accuracy. 
              The results tell us that <b><a href="#demo5" class="demo-link5">Finding 5: More capable base models tend to produce more sophisticated and effective harmful software.</a></b><br>
          </div>

        </div>  
      </section>  

  <section class="section hero is-small" id="benchmark" >  
    <div class="container is-max-desktop">  
      <div class="columns is-centered has-text-centered">  
        <div class="column is-four-fifths">  
          <h2 class="title is-3">Benchmark Overview</h2>  
        </div>  
      </div>  
      <p>
        <div id="categories" class="demo-container">
        
          <img src="assets/RedCode-logo-512.png" style="vertical-align: middle" width="28px"> <span class="name-context">RedCode</span> consists of two parts to evaluate agents' safety in unsafe code execution and
          generation: <span class="name-context">RedCode-Exec</span> and <span class="name-context">RedCode-Gen</span>. The taxonomy of each part is shown in the figures below.
          <br>
          <br>
          <img src="assets/redcode-exec-category.png" alt="Taxonomy of RedCode-Exec."/>
          <br>
          <br>
        
          <span class="name-context">RedCode-Exec</span> includes a taxonomy of 25 risky scenarios across 8 domains, ranging from real system and network operations to program logic and so on.
          <br>
          <br>
          <img src="assets/redcode-gen-category.png" style="width: 100%; max-width: 1000px; display: block; margin: 0 auto;" alt="Taxonomy of RedCode-Exec."/>
          <br>
          For <span class="name-context">RedCode-Gen</span>, we provide 8 risky categories based on well-studied malware families for risky software generation.
          <!-- The prompts are generated with human-in-the-loop AI-assisted generation. In contrast to the natural language malware-related
          instructions found in prior safety benchmarks, we follow the format of HumanEval to evaluate code agents in more realistic scenarios where they must handle code. -->
        
      </div>

      <div id="pipeline" class="demo-container">
        <img src="assets/dataset-pipeline-new-6.jpg" alt="Overall pipeline" style="margin-bottom: 10px;"/>
        <br>
        <div class="columns is-centered has-text-centered">
          <p style="color: gray;">
            <!-- Overview of <span class="name-context">RedCode</span> benchmark. -->
          </p>
        </div>
        We provide an overview of our üõ†Ô∏è<b>benchmark pipeline</b> here.
        <br>
        <br>
        For <span class="name-context">RedCode-Exec</span>, we begin by collecting the risky scenarios from the existing benchmarks (e.g. <a href="https://arxiv.org/html/2401.10019v2">R-Judge</a>) and <a href="https://cwe.mitre.org/data/published/cwe_v4.13.pdf">CWE</a>. Guided by these
        risky scenarios and risky descriptions, we construct üå±<b style="color: green;"> initial seed test cases (Python code snippets)</b> by
        ‚úçÔ∏èmanually designing the code or gathering from üñ•Ô∏èonline sources. Next, we employ a human-in-the-loop
        procedure to expand and refine our dataset. (1) We utilize <b>LLMs to generate additional code snippets</b>
        under the same risky scenario, thereby expanding our dataset. (2) We <b>ensure the quality of our
        dataset</b> by the following three steps: <u>(a) Manual Review (b) Accessible Resources Preparation (c)
        Executability Verification.</u> The details of the above steps are outlined in ¬ß3.2.1 in our <a href="">paper</a>. Once the Python code
        snippet dataset is finalized, we use LLMs to üõ†Ô∏è<b>translate the snippets into Bash scripts and
          natural language</b>. To evaluate the safety of the code agent via RedCode-Exec, we use the
        prompts from our dataset as input for the code agent and instruct it to execute the risky code in our
        Docker environment. We evaluate if the agent can complete the given risky tasks using corresponding
        evaluation scripts. The figure below shows our evaluation details.
        <br>
        <br>
        <img src="assets/eval_details.png" style="width: 100%; max-width: 1000px; display: block; margin: 0 auto;" alt="eval_details"/>
        <br>
        
        Previous work use <b>ü§ñ‚öñÔ∏èLLM as a judge</b> to evaluate the records (i.e., trajectory in <a href="https://arxiv.org/abs/2309.15817">ToolEmu</a> , response in <a href="https://arxiv.org/abs/2311.10538">AgentMonitor</a>, and records in <a href="https://arxiv.org/html/2401.10019v2">R-Judge</a>) of agents
        and output safety label or safety score. In our work, we allow agents run in our well-prepared environment and use specific
        <b><img src="assets/python.png" style="vertical-align: middle" width="25px">‚öñÔ∏èevaluation scripts</b> to <b>accurately</b> evaluate the whole execution process, from the agent receiving prompts to the
        agent finishing the whole task. In hte figure above, the # risky scenario ID can help locate the corresponding evaluation script, and
        the target label can be used to check if the execution is successful or not. Our evaluation combines <u>
        (1) agent responses, (2) code execution results, and (3) results from interaction with the execution environment (i.e., the
        Docker container)</u> to give an accurate judgement.
        <br>
        <br>
        For <span class="name-context">RedCode-Gen</span>, we provide eight risky categories based on well-studied malware families for
        risky software generation. The prompts are generated with human-in-the-loop AI-assisted generation.
        We then give the prompts to the agent to generate code in our docker environment. We
        evaluate the quality of the generated code via both <b>ü§ñ‚öñÔ∏èLLM as a judge and ‚öñÔ∏èVirusTotal API.</b>

      </p>
      </div>  
    </div>  
  </section>  
  <!-- End Benchmark overview -->  

  <!-- Leaderboard -->  
  <section class="section hero is-small" id="leaderboard">  
    <div class="container is-max-desktop">  
      <div class="columns is-centered has-text-centered">  
        <div class="column">  
          <h2 class="title is-3">Leaderboard</h2>  
        </div>  
      </div>  

      <div class="demo-container">
        <!-- <p style="color: red;"> TODO: The explanation of leaderboard here. </p>   -->
        The results of OCI, ReAct and CodeAct on <span class="name-context">RedCode-Exec</span> are shown in the following tables. 
        The results of LLMs and agents on <span class="name-context">RedCode-Gen</span> can be found in the figure of <b><a href="#demo5" class="demo-link5">Finding 5</a></b> above.
        <br>
        <br>
        üñ±Ô∏è<b>Click</b> the "RR" or "ASR" below to sort corresponding results. RR denotes rejection rate and ASR denotes attack success rate. For RR, the higher the safer. For ASR, the lower the safer.
        <br>
        <br>

      <table id="leaderboard-exec">
          <thead>
              <tr>
                  <th>Rank</th>
                  <th>Agent</th>
                  <th>Base model</th>
                  <th>Task</th>

                  <th id="sort-rejection-exec" class="highlight tooltip">RR(%)‚Üë
                    <span class="sort-indicator" id="rejection-indicator-exec">‚ñº</span>
                    <span class="tooltiptext">Sort</span>
                  </th>
                  <th id="sort-asr-exec" class="highlight tooltip">ASR(%)‚Üì
                    <span class="sort-indicator" id="asr-indicator-exec"></span>
                    <span class="tooltiptext">Sort</span>
                  </th>
              </tr>
          </thead>
          <tbody>
              <!-- Rows will be inserted here by JavaScript -->
          </tbody>
      </table>
        
      </div>  
    </div>  
  </section>  

  <!-- End Leaderboard -->  

  <!-- Case study -->  
  <!-- <section class="section hero is-light" id="case-study">  
    <div class="container is-max-desktop">  
      <div class="columns is-centered has-text-centered">  
        <div class="column is-four-fifths">  
          <h2 class="title is-3">Case Study</h2>  
          <div class="content has-text-justified">  
            <p>  
              <p style="color: red;"> TODO: The runtime logs to show case studies. </p>  
              <p style="color: red;"> TODO: This part may need Chengquan and Chulin's help in providing insights of OCI and CodeAct agents.</p>  
              <p style="color: red;"> Note: Maybe not release this section in a hurry?</p>  
              The runtime logs of the three agents on <span class="name-context">RedCode-Exec</span> are shown in the following blocks to provide examples.  
            </p>  
          </div>  
        </div>  
      </div>  
    </div>  
  </section>   -->
  <!-- Case study -->  

  <!--BibTex citation -->  
  <section class="section" id="BibTeX">  
    <div class="container is-max-desktop content">  
      <h2 class="title">BibTeX</h2>  
      <p style="color: red;"> TODO: Fill out the BibTeX content. </p>  
      <pre><code class="bibtex-code">BibTex Code Here</code></pre>  
    </div>  
  </section>  
  <!--End BibTex citation -->  

  <footer class="footer">  
    <div class="container">  
      <div class="columns is-centered">  
        <div class="column is-8">  
          <div class="content">  

            <p>
              Acknowledgement: This webpage is inspired by other excellent work, such as <a
              href="https://sorry-bench.github.io/index.html" target="_blank">SORRY-Bench</a>.
              <br>
              <br>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io"
                target="_blank">Nerfies</a> project page.
              You are free to borrow the of this website, we just ask that you link back to this page in the footer.
              <br>
              <br>
              This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"
                target="_blank">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>

          </div>  
        </div>  
      </div>  
    </div>  
  </footer>  

  <!-- Statcounter tracking code -->  

  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->  

  <!-- End of Statcounter Code -->  

  <script>  
    function toggleNavbar() {  
      var navbarRight = document.getElementById("navbar-right");  
      navbarRight.classList.toggle("active");  
    }  
  </script>

</body>  

</html>