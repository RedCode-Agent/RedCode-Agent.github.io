<!DOCTYPE html>  
<html>  

<head>  
  <meta charset="utf-8">  
  <meta name="google-site-verification" content="lTNLmXZvHm_RXq9IklCb7g-0DA_3JIIpvcV9Um-vpRQ" />
  <meta name="description" content="RedCode Benchmark Official Webpage">  
  <meta property="og:title" content="RedCode" />  
  <meta property="og:description" content="A Risky Code Execution and Generation Benchmark for Code Agents" />  

  <title>RedCode</title>  
  <link rel="icon" type="image/x-icon" href="assets/RedCode-logo-1024.ico">  
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">  
  <link href="https://fonts.googleapis.com/css2?family=Nanum+Gothic+Coding:wght@400;700&display=swap" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css2?family=Ubuntu+Mono&display=swap" rel="stylesheet">  
  <link href="https://fonts.googleapis.com/css2?family=Chivo+Mono:ital,wght@0,100..900;1,100..900&display=swap" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">  
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">  
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">  
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">  
  <link rel="stylesheet" href="static/css/index.css">  
  <link rel="stylesheet" href="static/css/navbar.css">  
  <link rel="stylesheet" href="static/css/redcode-font.css">  
  <link rel="stylesheet" href="static/css/leaderboard.css">  

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>  
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>  
  <script defer src="static/js/fontawesome.all.min.js"></script>  
  <script src="static/js/bulma-carousel.min.js"></script>  
  <script src="static/js/bulma-slider.min.js"></script>  
  <script src="static/js/index.js"></script>  

</head>  

<body>  

  <nav class="navbar"> 
    <div class="navbar-left">
        <a href="#motivation">
            <img src="assets/RedCode-logo-512.png" alt="RedCode Logo">
            <span class="name-bold">RedCode</span>
        </a>
    </div>

    <div class="navbar-right" id="navbar-right">
        <a href="#motivation">Motivation</a>
        <a href="#abstract">Abstract</a>
        <a href="#benchmark">Overview</a>
        <a href="#leaderboard">Leaderboard</a>
        <a href="#results">Results Demo</a>
        <a href="#case-study">Case Study</a>
        <a href="#BibTeX">Cite us</a>
    </div>
    
    <div class="menu-icon">
        <i class="fa fa-bars"></i>
    </div>
  </nav>


  <section class="hero" id="motivation">  
    <div class="hero-body">  
      <div class="container is-max-desktop">  
        <div class="columns is-centered">  
          <div class="column has-text-centered">  
            <h1 class="title is-1 publication-title"><img src="assets/RedCode-logo-512.png" style="vertical-align: middle" width="50px"> <span class="name-bold" >RedCode</span>: Risky Code Execution and Generation Benchmark for Code Agents</h1>  
            <div class="is-size-5 publication-authors">  
              <!-- Paper authors -->  
              <span class="author-block">  
                <a href="https://github.com/1mocat" target="_blank">Chengquan Guo</a><sup>1*</sup>,</span>  
              <span class="author-block">  
                <a href="https://antiquality.github.io" target="_blank">Xun Liu</a><sup>2*</sup>,</span>  
              <span class="author-block">  
                <a href="https://alphapav.github.io" target="_blank">Chulin Xie</a><sup>2*</sup>,</span>  
              <br>  
              <span class="author-block">  
                <a href="https://www.andyzhou.ai" target="_blank">Andy Zhou</a><sup>2,3</sup>,</span>  
              <span class="author-block">  
                <a href="https://www.yi-zeng.com" target="_blank">Yi Zeng</a><sup>4</sup>,</span>  
              <span class="author-block">  
                <a href="https://zinanlin.me" target="_blank">Zinan Lin</a><sup>5</sup>,</span>  
              <span class="author-block">  
                <a href="https://dawnsong.io" target="_blank">Dawn Song</a><sup>6</sup>,</span>  
              <span class="author-block">  
                <a href="https://aisecure.github.io" target="_blank">Bo Li</a><sup>1,2</sup>  
              </span>  
            </div>  

            <div class="is-size-5 publication-authors">  
              <span class="author-block">  
                <small>  
                  <sup>1</sup>University of Chicago
                  <sup>2</sup>University of Illinois Urbana-Champaign 
                  <sup>3</sup>Lapis Labs  
                  <br> 
                  <sup>4</sup>Virginia Tech
                  <sup>5</sup>Microsoft Corporation  
                  <sup>6</sup>UC Berkeley
                </small>  
              </span>  
              <span class="eql-cntrb"><small><br><sup>*</sup> Equal Contribution.</small> <small>Work done during Chengquan's internship at the University of Chicago and <br> Xun's internship at the University of Illinois Urbana-Champaign.</small></span>  
            </div>  

            <p style="color: red;"> TODO: Fill out the link in Paper and arXiv block. </p>  

            <div class="column has-text-centered">  
              <div class="publication-links">  
                <!-- Arxiv PDF link -->  
                <span class="link-block">  
                  <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank" class="external-link button is-normal is-rounded is-dark">  
                    <span class="icon">  
                      <i class="fas fa-file-pdf"></i>  
                    </span>  
                    <span>Paper</span>  
                  </a>  
                </span>  

                <!-- ArXiv abstract Link -->  
                <!-- <span class="link-block">  
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank" class="external-link button is-normal is-rounded is-dark">  
                    <span class="icon">  
                      <i class="ai ai-arxiv"></i>  
                    </span>  
                    <span>arXiv</span>  
                  </a>  
                </span>   -->

                <!-- Github link -->  
                <span class="link-block">  
                  <a href="https://github.com/RedCode-2024/RedCode" target="_blank" class="external-link button is-normal is-rounded is-dark">  
                    <span class="icon">  
                      <i class="fab fa-github"></i>  
                    </span>  
                    <span>Code</span>  
                  </a>  
                </span>  

                <!-- Leaderboard link -->
                <span class="link-block">  
                  <a href="#leaderboard" class="external-link button is-normal is-rounded is-dark">  
                    <span class="icon">
                      <i class="fa fa-trophy"></i>
                    </span>
                    <span>Leaderboard</span>  
                  </a>  
                </span> 

                <!-- Dataset link -->  
                <span class="link-block">  
                  <a href="https://huggingface.co/datasets/AI-Secure/RedCode" target="_blank" class="external-link button is-normal is-rounded is-dark">  
                    <span class="icon">
                        <img src="assets/huggingface-logo.svg" alt="Hugging Face Logo" width="250" height="250">  
                    </span>
                    <span>Dataset</span>  
                  </a>  
                </span>  
              </div>  
            </div>  
          </div>  
        </div>  
      </div>  
    </div>  
  </section>  
  <!-- Teaser video-->  
  <!-- <section class="hero teaser">  
    <div class="container is-max-desktop">  
      <div class="hero-body">  
        <h2 class="subtitle has-text-centered">  
          <div class="content has-text-justified">  
            <p>
            Integrated with external tools like Python interpreters or shell environments, LLM-based code agents have significantly advanced AI-assisted coding and software development.
            However, despite their impressive capabilities, these ‚ö†Ô∏è<b style="color: rgb(255, 115, 0);">code agents are not risk-free</b>.
            <b><i>Code agents can inadvertently suggest or execute code with security vulnerabilities like deleting important files or leaking sensitive information.</i></b>
            <br>üéØTo rigorously and comprehensively evaluate the safety of code agents, 
            we propose <img src="assets/RedCode-logo-512.png" style="vertical-align: middle" width="28px"><b class="name-bold"> RedCode</b>
            , a <u>high-quality, large-scale</u> dataset that features <u>diverse languages and formats</u>, providing <u>real interaction with systems</u> and <u>fine-grained evaluation</u> of both <u>code execution and generation</u>.
            </p>
          </div>
          <br>  
          Our experiment reveals that
          <br>  
          <b>OpenCodeInterpreter is safer than ReAct and CodeAct agents.</b>  
          <br>  
          <p style="color: red;"> TODO: Other impressive slogan? </p>  
          <img src="assets/fig-1(1x1).png" alt="Fig-1:scatter of REJ-ASR." style="width: 80%; height: auto;"/>  
          <br>
        </h2>  
        <div class="columns is-centered has-text-centered">  
          <p style="color: gray;">
            Safety evaluation of 14 code agents on <span class="name-context">RedCode-Exec</span>, where the color of the datapoint
            denotes agent type and the label "A(B)" denotes the results of this agent with base LLM A under risky test cases in
            language B, and * denotes fine-tuned LLMs for agent. The more upper left the datapoint is located, the safer the agent.
            In general, OpenCodeInterpreter series (the yellow ones) are the safest agents, ReAct series (the red ones) are in the
            middle, and CodeAct series (the blue ones) are the most unsafe.
            Different base LLMs significantly affect the agent's safety.
            <br>
            <span style="color: red;"> TODO: Do we also need to cite every figure in the webpage, like what we did in paper? </span>  
          </p>
        </div>
      </div>  
    </div>  
  </section>   -->
  <!-- End teaser video -->  

  <section class="section hero is-small" id="benchmark">  
    <div class="container is-max-desktop">  
      <div class="columns is-centered has-text-centered">  
        <div class="column is-four-fifths">  
          <h2 class="title is-3" style="margin-top: -80px;">Motivation</h2>  
            <div class="content has-text-justified">  
              <p>
              Integrated with external tools like Python interpreters or shell environments, LLM-based code agents have significantly advanced AI-assisted coding and software development.
              However, despite their impressive capabilities, these ‚ö†Ô∏è<b style="color: rgb(255, 115, 0);">code agents are not risk-free</b>.
              <b><i>Code agents can inadvertently suggest or execute code with security vulnerabilities like deleting important files or leaking sensitive information.</i></b>
              <br>üéØTo rigorously and comprehensively evaluate the safety of code agents, 
              we propose <img src="assets/RedCode-logo-512.png" style="vertical-align: middle" width="28px"><b class="name-bold"> RedCode</b>
              , a <u>high-quality, large-scale</u> dataset that features <u>diverse languages and formats</u>, providing <u>real interaction with systems</u> and <u>fine-grained evaluation</u> of both <u>code execution and generation</u>.
              <br>
              <span class="name-context">RedCode</span> consists of <span class="name-context">RedCode-Exec</span> and <span class="name-context">RedCode-Gen</span>.<br>
              <span class="name-context">RedCode-Exec</span> provides prompts to evaluate code agents' ability to recognize and handle unsafe code with a total of over 4,000 testing instances.
              <span class="name-context">RedCode-Gen</span> provides 160 prompts with function signatures as input to assess whether code agents will follow instructions to generate harmful code or software.
              </p>
            </div>
          </h2>  
          


          </div>  
        </div>  
      </div>  
    </div>  
  </section>  


    <!-- Results Demo -->
    <script src="https://d3js.org/d3.v7.min.js"></script>
    <script src="static/js/leaderboard.js"></script>
    
    <section class="section hero is-small" id="results">  
      <div class="container is-max-desktop">  
        <div class="columns is-centered has-text-centered">  
          <div class="column is-four-fifths">  
            <h2 class="title is-3" style="margin-top: -80px;">Results Demo</h2>  
            <div class="content has-text-justified">
              <b style="color:cornflowerblue;">Our experiment reveals that</b>
              <br>
              <b><a href="#demo1" class="demo-link1">1.OpenCodeInterpreter is safer than ReAct and CodeAct agents.</a></b><br>
              <b><a href="#demo2" class="demo-link2">2.Agents are more likely to reject executing unsafe operations in the operating system domain.</a></b><br>
              <b><a href="#demo3" class="demo-link3">3.Agents are less likely to reject risky queries in natural language than programming language inputs, or in Bash code than Python code inputs.</a></b><br>
              <b><a href="#demo4" class="demo-link4">4.Evaluations on RedCode-Exec reveals that more capable base models, such as GPT-4, have a higher rejection rate for unsafe operations.</a></b><br>
              <b><a href="#demo5" class="demo-link5">5.Evaluations on RedCode-Gen reveals that more capable base models tend to produce more sophisticated and effective harmful software.</a></b><br><br>
              <div id="demo1" class="demo-container">
                  <img src="assets/fig-1(1x1).png" alt="Fig-1:scatter of REJ-ASR."/>
                  <p>
                      <b>Demo 1: </b>Safety evaluation of 14 code agents on <span class="name-context">RedCode-Exec</span>, where the color of the datapoint
                      denotes agent type and the label "A(B)" denotes the results of this agent with base LLM A under risky test cases in
                      language B, and * denotes fine-tuned LLMs for agent. The more upper left the datapoint is located, the safer the agent.
                      In general, OpenCodeInterpreter series (the yellow ones) are the safest agents, ReAct series (the red ones) are in the
                      middle, and CodeAct series (the blue ones) are the most unsafe.
                      Different base LLMs significantly affect the agent's safety.
                      <br>
                      <!-- <span style="color: red;"> TODO: Do we also need to cite every figure in the webpage, like what we did in paper? </span>   -->
                  </p>
              </div>
              <br>
                <div id="demo2" class="demo-container">
                  <img src="assets/radar-high-level.jpg" alt="Radar figure of RedCode-Exec."/>
                  <!-- <p style="color: gray;"> -->
                    <!-- <span style="color: red;"> TODO: The titles of radar figure are better to be capitalized in the first alphabet.</span>   -->
                    <br>
                    <b>Demo 2: </b>Reject and attack success rates of 8 representative code agents on <span class="name-context">RedCode-Exec</span> Python test cases
                    in 8 domains. The overall attack success rate is high, indicating existing code agents are vulnerable. Agents
                    achieve a higher rejection rate on risky cases in operating system and file system than those in other domains.
                  <!-- </p> -->
                </div>
                <!-- <p style="color: red;"> TODO: More analysis for the results figures.</p>   -->
                <!-- <img src="assets/heatmap.jpg" alt="Heatmap of RedCode-Exec."/> -->
                <div id="demo3"  class="demo-container">
                  <img src="assets/demo3.png" style="width: 100%; max-width: 600px; display: block; margin: 0 auto;" alt="Radar figure of RedCode-Exec." />
                  <!-- <p style="color: gray;"> -->
                    <!-- <span style="color: red;"> TODO: The titles of radar figure are better to be capitalized in the first alphabet.</span>   -->
                    <br>
                    <b>Demo 3: </b> <span class="name-context">RedCode-Exec</span> evaluation on
                    ReAct{GPT-4} over various risky inputs. Agents are less likely to reject risky queries in natural language than programming language inputs, or in Bash code than Python code inputs.
                  <!-- </p> -->
                </div>
                
                <div id="demo4"></div>
                <div id="heatmap"></div>
                <div id="tooltip"
                  style="position: absolute; display: none; padding: 10px; background: rgba(0, 0, 0, 0.6); color: white; border-radius: 5px; pointer-events: none; z-index: 10;">
                </div>
                <script src="static/js/heatmap_no_sel.js"></script>
                <b>Demo 4: </b>Attack success rates for <span class="name-context">RedCode-Exec</span> across 14 different agents. * denotes fine-tuned LLMs for corresponding agents.

                
                <p style="color: red;"> TODO: Padding details of the following H5 heatmap</p>  
                <p style="color: red;"> Note: This H5 heatmap only contains the ASR on Python tasks, so the number is few.</p>  
                <p style="color: red;"> TODO: Need Rej and Bash heatmap</p>  

                <div id="demo5"  class="demo-container">
                  <img src="assets/demo5.png" style="width: 100%; max-width: 600px; display: block; margin: 0 auto;" alt="Radar figure of RedCode-Exec."/>
                  <!-- <p style="color: gray;"> -->
                    <!-- <span style="color: red;"> TODO: The titles of radar figure are better to be capitalized in the first alphabet.</span>   -->
                    <br>
                    <b>Demo 5: </b>Overall results for base models and code agents on <span class="name-context">RedCode-Gen</span>. We find low refusal rates and high accuracy in this setting for most base models. Code agents have lower refusal rates and higher accuracy.
                  <!-- </p> -->
                </div>

                <p>
                  <!-- <p style="color: red;"> TODO: This heatmap needs Chengquan's update.</p>   -->
                  <!-- <p style="color: red;"> TODO: The art of this heatmap definitely needs to polish.</p>   -->
                </p>
              </div>  
            </div>  
          </div>  
  

        </div>  
      </section>  
      
    <!-- Results Demo -->  

  <!-- Paper abstract -->  
  <!-- <section class="section hero is-light" id="abstract">  
    <div class="container is-max-desktop">  
      <div class="columns is-centered has-text-centered">  
        <div class="column is-four-fifths">  
          <h2 class="title is-3">Abstract</h2>  
          <div class="content has-text-justified">  
            <p>  
              <p style="color: red;"> TODO: The diction of TL;DR needs review. e.g. Are we "the first one to blablabla"?</p>  
              <p>
                <b>TL;DR: Our high-quality large-scale dataset offers holistic evaluation on Code Agents, in diverse languages and formats.</b>
              </p>
              With the rapidly increasing capabilities and adoption of code agents for AI-assisted coding and software development, safety and security concerns - such as generating or executing malicious code - have become significant barriers to the real-world deployment of these agents. To provide comprehensive and practical evaluations on the safety of code agents, we propose <span class="name-context">RedCode</span>,  
              an evaluation platform with benchmarks grounded in four key principles - real interaction with systems, holistic evaluation of unsafe code generation and execution, diverse input formats, and high-quality safety scenarios and tests.  
              <span class="name-context">RedCode</span> consists of two parts to evaluate agents' safety in unsafe code execution and generation:  
              <b>  
              (1) <span class="name-context">RedCode-Exec</span> provides challenging code prompts in Python as inputs, aiming to evaluate code agents' ability to recognize and handle unsafe code. We then map the Python code to other programming languages (e.g., Bash) and natural text summaries or descriptions for evaluation, leading to a total of over 4,000 testing instances.  
              </b>  
              We provide 25 types of critical vulnerabilities spanning various domains, such as websites, file systems, and operating systems.  
              We provide a Docker sandbox environment to evaluate the execution capabilities of code agents and design corresponding evaluation metrics to assess their execution results.  
              <b>  
              (2) <span class="name-context">RedCode-Gen</span> provides 160 prompts with function signatures as input to assess whether code agents will follow instructions to generate harmful code or software.  
              </b>  
              Our empirical findings, derived from evaluating three agents based on various LLMs, provide insights into code agents' vulnerabilities. For instance,  evaluations on <span class="name-context">RedCode-Exec</span> show that agents are more likely to reject executing unsafe operations on operating system. Unsafe operations described in natural text lead to a lower rejection rate than those in code format.  
              Additionally,  evaluations on <span class="name-context">RedCode-Gen</span> reveal that  
              more capable base models and agents with stronger overall coding abilities, such as GPT-4, tend to produce more sophisticated and effective harmful software.  
              Our findings highlight the need for stringent safety evaluations for diverse code agents.  
            </p>  
          </div>  
        </div>  
      </div>  
    </div>  
  </section>   -->
  <!-- End paper abstract -->  

  <!-- Benchmark overview -->  
  <section class="section hero is-small" id="benchmark">  
    <div class="container is-max-desktop">  
      <div class="columns is-centered has-text-centered">  
        <div class="column is-four-fifths">  
          <h2 class="title is-3">Benchmark Overview</h2>  
          <div class="content has-text-justified">  
            <p>
              <p>
                <span class="name-context">RedCode</span> consists of two parts to evaluate agents' safety in unsafe code execution and
                generation: <span class="name-context">RedCode-Exec</span> and <span class="name-context">RedCode-Gen</span>. The taxonomy of each part is shown in the figures below.
              </p>

              <img src="assets/redcode-exec-category.png" alt="Taxonomy of RedCode-Exec."/>
              <br>
              <!-- <div class="columns is-centered has-text-centered">
                <p style="color: gray;">
                  Taxonomy of 25 risky scenarios spanning 8 domains in <span class="name-context">RedCode-Exec</span>, ranging from real system, network operations to program logic and so on.
                </p>
              </div> -->

              <p>
                <span class="name-context">RedCode-Exec</span> includes a taxonomy of 25 risky scenarios across 8 domains, ranging from real system and network operations to program logic and so on.
                <!-- <p style="color: red;"> TODO: Add some descriptions of the taxonomy.</p>   -->
                <br>
              </p>

              <img src="assets/redcode-gen-category.png" style="width: 100%; max-width: 1000px; display: block; margin: 0 auto;" alt="Taxonomy of RedCode-Exec."/>
              <br>
              <p>
                <br>
                For <span class="name-context">RedCode-Gen</span>, we provide 8 risky categories based on well-studied malware families for risky software generation.
                <!-- The prompts are generated with human-in-the-loop AI-assisted generation. In contrast to the natural language malware-related
                instructions found in prior safety benchmarks, we follow the format of HumanEval to evaluate code agents in more realistic scenarios where they must handle code. -->
              </p>

              <img src="assets/dataset-pipeline-new-6.jpg" alt="Scatter of REJ-ASR." style="margin-bottom: 10px;"/>
              <br>
              <div class="columns is-centered has-text-centered">
                <p style="color: gray;">
                  <!-- Overview of <span class="name-context">RedCode</span> benchmark. -->
                </p>
              </div>
              We provide an overview of our benchmark RedCode pipeline here. <br>
              For <span class="name-context">RedCode-Exec</span>, we begin by collecting the risky scenarios from the existing benchmarks (e.g. <a href="https://arxiv.org/html/2401.10019v2">R-Judge</a>) and <a href="https://cwe.mitre.org/data/published/cwe_v4.13.pdf">CWE</a>. Guided by these
              risky scenarios and risky descriptions, we construct üå±<b style="color: rgb(0, 136, 72);"> initial seed test cases (python code snippets)</b> by
              ‚úçÔ∏èmanually designing the code or gathering from üñ•Ô∏èonline sources. Next, we employ a human-in-the-loop
              procedure to expand and refine our dataset. (1) We utilize <b>LLMs to generate additional code snippets</b>
              under the same risky scenario, thereby expanding our dataset. (2) We <b>ensure the quality of our
              dataset</b> by the following three steps: <u>(a) Manual Review (b) Accessible Resources Preparation (c)
              Executability Verification.</u> The details of the above steps are outlined in ¬ß3.2.1 in our <a href="https://arxiv.org/html/2401.10019v2">paper</a>. Once the Python code
              snippet dataset is finalized, we use LLMs to üõ†Ô∏è<b>translate the snippets into Bash scripts and
                natural language</b>. To evaluate the safety of the code agent via RedCode-Exec, we use the
              prompts from our dataset as input for the code agent and instruct it to execute the risky code in our
              Docker environment. We evaluate if the agent can complete the given risky tasks using corresponding
              evaluation scripts. The figure below shows our evaluation details.<br>
              <img src="assets/eval_details.png" style="width: 100%; max-width: 1000px; display: block; margin: 0 auto;" alt="eval_details"/>
              
              Evaluation comparison between previous work and ours. Previous work use LLM as a judge to
              evaluate the records (i.e., trajectory in <a href="https://arxiv.org/abs/2309.15817">ToolEmu</a> , response in <a href="https://arxiv.org/abs/2311.10538">AgentMonitor</a>, and records in <a href="https://arxiv.org/html/2401.10019v2">R-Judge</a>) of agents
              and output safety label or safety score. We allow agents run in our well-prepared environment and use specific
              evaluation scripts to accurately evaluate the whole execution process, from the agent receiving prompts to the
              agent finishing the whole task. The # risky scenario ID can help locate the corresponding evaluation script, and
              the target label can be used to check if the execution is successful or not. Our evaluation combines (1) agent
              responses, (2) code execution results, and (3) results from interaction with the execution environment (i.e., the
              Docker container) to give an accurate judgement.

              For RedCode-Gen, we provide eight risky categories based on well-studied malware families for
              risky software generation. The prompts are generated with human-in-the-loop AI-assisted generation.
              We then give the prompts to the agent to generate code in our docker environment. We
              evaluate the quality of the generated code via both LLM-as-a-judge and VirusTotal API.

            </p>
          </div>  
        </div>  
      </div>  
    </div>  
  </section>  
  <!-- End Benchmark overview -->  

  <!-- Leaderboard -->  
  <section class="section hero is-light" id="leaderboard">  
    <div class="container is-max-desktop">  
      <div class="columns is-centered has-text-centered">  
        <div class="column is-four-fifths">  
          <h2 class="title is-3">Leaderboard</h2>  
          <div class="content has-text-justified">  
            <p>  
              The results of OCI, ReAct and CodeAct on <span class="name-context">RedCode-Exec</span> and <span class="name-context">RedCode-Gen</span> are shown in the following tables. <span style="color: red;"> TODO: Add RedCode-Gen results here; reference to MINT. </span>  
            </p>  

            <table id="leaderboard-exec">
                <thead>
                    <tr>
                        <th>Rank</th>
                        <th>Model</th>
                        <th>Agent</th>
                        <th>Task</th>

                        <th id="sort-rejection-exec" class="highlight tooltip">Rejection(%) ‚Üë
                          <span class="sort-indicator" id="rejection-indicator-exec">‚ñº</span>
                          <span class="tooltiptext">Sort</span>
                        </th>
                        <th id="sort-asr-exec" class="highlight tooltip">ASR(%) ‚Üì
                          <span class="sort-indicator" id="asr-indicator-exec"></span>
                          <span class="tooltiptext">Sort</span>
                        </th>
                    </tr>
                </thead>
                <tbody>
                    <!-- Rows will be inserted here by JavaScript -->
                </tbody>
            </table>

            <p style="color: red;"> TODO: The explanation of leaderboard here. </p>  
          </div>  
        </div>  
      </div>  
    </div>  
  </section>  

  <!-- End Leaderboard -->  

  <!-- Case study -->  
  <!-- <section class="section hero is-light" id="case-study">  
    <div class="container is-max-desktop">  
      <div class="columns is-centered has-text-centered">  
        <div class="column is-four-fifths">  
          <h2 class="title is-3">Case Study</h2>  
          <div class="content has-text-justified">  
            <p>  
              <p style="color: red;"> TODO: The runtime logs to show case studies. </p>  
              <p style="color: red;"> TODO: This part may need Chengquan and Chulin's help in providing insights of OCI and CodeAct agents.</p>  
              <p style="color: red;"> Note: Maybe not release this section in a hurry?</p>  
              The runtime logs of the three agents on <span class="name-context">RedCode-Exec</span> are shown in the following blocks to provide examples.  
            </p>  
          </div>  
        </div>  
      </div>  
    </div>  
  </section>   -->
  <!-- Case study -->  

  <!--BibTex citation -->  
  <section class="section" id="BibTeX">  
    <div class="container is-max-desktop content">  
      <h2 class="title">BibTeX</h2>  
      <p style="color: red;"> TODO: Fill out the BibTeX content. </p>  
      <pre><code class="bibtex-code">BibTex Code Here</code></pre>  
    </div>  
  </section>  
  <!--End BibTex citation -->  

  <footer class="footer">  
    <div class="container">  
      <div class="columns is-centered">  
        <div class="column is-8">  
          <div class="content">  

            <p>
              Acknowledgement: This webpage is inspired by other excellent work, such as <a
              href="https://sorry-bench.github.io/index.html" target="_blank">SORRY-Bench</a>.
              <br>
              <br>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io"
                target="_blank">Nerfies</a> project page.
              You are free to borrow the of this website, we just ask that you link back to this page in the footer.
              <br>
              <br>
              This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"
                target="_blank">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>

          </div>  
        </div>  
      </div>  
    </div>  
  </footer>  

  <!-- Statcounter tracking code -->  

  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->  

  <!-- End of Statcounter Code -->  

  <script>  
    function toggleNavbar() {  
      var navbarRight = document.getElementById("navbar-right");  
      navbarRight.classList.toggle("active");  
    }  
  </script>

</body>  

</html>